{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f444dc57-7b82-4ee2-a78a-feb35fbd392a",
   "metadata": {},
   "source": [
    "# Identify (Users and/or Tweets) to Filter\n",
    "\n",
    "**Authors:** Simon Todd & Chloe Willis  \n",
    "**Date:** July 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfee23-7365-4c3e-9d14-fc9ef229e7f3",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to identify Tweets that need to be removed from the dataset on the basis of (1) the user who posted the Tweet or (2) the content of the Tweet. These Tweets are not removed from the dataset in this notebook; rather, they are labeled here according to the level at which they would be filtered out, to facilitate the pairing of Tweets in the study and reference corpora by timebin and filter level in `align_corpora.py`. The actual filtering occurs in the calculation of keyness scores, in `score_keyness.py`.\n",
    "\n",
    "We have included this as a notebook rather than a standalone .py file because the criteria for filtering are dependent upon your purpose, and are often defined interactively through data exploration. You may use separate criteria to label your Tweets for level of filtering; labels will be carried across for pairing in `align_corpora.py`, and can be flexibly provided when calculating keyness scores in `score_keyness.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7a16a-82b7-4ec0-bd4e-f56b52ac5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import glob\n",
    "from collections import Counter\n",
    "from count_tweet_words import extract_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ba6b4-de12-4b3a-bec3-0031c225e551",
   "metadata": {},
   "source": [
    "## Filtering on the basis of users\n",
    "\n",
    "To facilitate filtering on the basis of users, we will keep a set of users that we need to filter out. We'll update this set as we build our filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a731525-98b2-46f7-aa08-f5150cddfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_filter = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446113f3-e3f7-47ad-b44a-69b143cf78ad",
   "metadata": {},
   "source": [
    "### Filtering out bots using tweetbotornot2\n",
    "\n",
    "In our analysis, we used [tweetbotornot2](https://github.com/mkearney/tweetbotornot2) to evaluate the probability that each unique user in our dataset was a bot, based on characteristics of their profile and most recent 200 Tweets. This section demonstrates how the output of tweetbotornot2 can be used to filter out likely bots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70264fc9-fb20-4adc-9cdf-4a4a7ee99f4b",
   "metadata": {},
   "source": [
    "First, we load the tweetbotornot2 output. We don't need the user ID column, since our harvested Tweets store usernames directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce169595-b85e-499c-85ab-a02f49714e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the tweetbotornot2 output file\n",
    "tweetbotornot_output_path = \"path/to/your/file.csv\"\n",
    "tweetbotornot_output = pd.read_csv(tweetbotornot_output_path, usecols=[\"screen_name\", \"prob_bot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38006afa-86a2-40b5-8dc6-7bcf3e6c1035",
   "metadata": {},
   "source": [
    "We need to identify a cutoff point for deciding that a user is a bot. To facilitate this, we can plot a histogram of bot probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f106bfe-4bc0-432b-bcbf-c2d0ecc50428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetbotornot_output.hist(column=\"prob_bot\", bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c90d1f-a2b3-483f-982c-6b3e6dcb21ba",
   "metadata": {},
   "source": [
    "In our data, the vast majority of users are assessed by tweetbotornot2 to have a probability of 0.1 or lower of being a bot, while there is a pocket of users concentrated around probability 0.99 of being a bot. We decided to label for filtering all users with a bot probability of 0.9 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccb840-48e7-407d-8f05-62fd01740084",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_threshold = 0.9 # Can be changed\n",
    "users_to_filter.update(tweetbotornot_output.loc[tweetbotornot_output[\"prob_bot\"] >= prob_threshold][\"screen_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21354f-56c6-4d31-9056-0341fc44e957",
   "metadata": {},
   "source": [
    "### Filtering out templatic users on the basis of type:token ratio\n",
    "\n",
    "If bot-detection is not available (e.g. because of changes in the Twitter API), or is not extensive enough, users can be further filtered on the basis of the type:token ratio (TTR) shown in Tweets on their timeline. A low TTR indicates less lexical variation, meaning that the user posts many Tweets that are templatic. This may be because the user is actually a bot, because they frequently use automation to generate Tweets (e.g. using Instagram or Etsy to auto-generate Tweets), and/or because they tend to post spam. We aim to exclude users whose TTR is too low.\n",
    "\n",
    "For our analysis, we harvested the 200 most recent Tweets from each user's timeline using the `rtweet` library in `R`, which is invoked in [tweetbotornot2](https://github.com/mkearney/tweetbotornot2) (see the `tweetbotornot.R` script). We note that timelines can also be harvested through [Twarc2 in Python](https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/#timelines).\n",
    "\n",
    "The function below calculates TTR for all unique users whose timelines have been harvested, and saves the results to a CSV file. In this calculation, it:  \n",
    "- normalizes words by removing punctuation and lowercasing (except for the @ and # characters, which designate mentions and hashtags, respectively);  \n",
    "- treats emoji as regular words, and separates emoji that are written as a sequence with no spaces;  \n",
    "- treats all media as tokens of a single type (`\"MEDIA\"`), and also calculates the number of media in the user's Tweets;  \n",
    "- treats all (non-media) links as tokens of a single type (`\"LINK\"`), and also calculates the number of links in the user's Tweets;  \n",
    "- also calculates a separate TTR over hashtags only, to see if the user repeatedly posts the same hashtags. \n",
    "\n",
    "The function assumes that the timelines have been saved in CSV format. Users may be batched across different files, but all of the Tweets for a single user are contained within consecutive rows of a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900788bd-fc4b-4548-94a0-d2f7028a1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token(input_csv_filepaths, output_csv_filepath,\n",
    "              username_col=\"screen_name\", text_col=\"text\",\n",
    "              hashtag_col=\"hashtags\", media_col=\"media_expanded_url\",\n",
    "              links_col=\"urls_expanded_url\"):\n",
    "    \"\"\"Writes a CSV file containing type:token ratio information for each \n",
    "    user in CSV files containing recent Tweets from users' timelines.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_csv_filepaths: list(str); filepaths where your timeline data is stored \n",
    "                         (see use of glob.glob below)\n",
    "    output_csv_filepath: str; desired file name & filepath for output CSV\n",
    "    username_col: str (default: \"screen_name\"); name of the column in input CSVs\n",
    "                  where usernames are stored\n",
    "    text_col: str (default: \"text\"); name of the column in input CSVs where\n",
    "              where Tweet text is stored\n",
    "    hashtag_col: str (default: \"hashtags\"); name of the column in input CSVs\n",
    "                 where hashtags are stored\n",
    "    media_col: str (default: \"media_expanded_url\"); name of the column in input\n",
    "               CSVs where media URLs are stored\n",
    "    links_col: str (default: \"urls_expanded_url\"); name of the column in input\n",
    "               CSVs where URLs of links in the Tweet are stored\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_csv_filepath, \"w\", encoding=\"utf-8\", newline=\"\") as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow([\"USERNAME\", \"TYPE_COUNT\", \"TOKEN_COUNT\", \"RATIO\", \"TOTAL_TWEETS\", \n",
    "                         \"HASHTAG_TYPES\", \"HASHTAG_TOKENS\", \"HASHTAG_RATIO\", \n",
    "                         \"MEDIA_COUNT\", \"LINK_COUNT\"])\n",
    "    \n",
    "        # Iterate through timeline files\n",
    "        for input_csv_filepath in input_csv_filepaths:\n",
    "            with open(input_csv_filepath, encoding=\"utf-8\", newline=\"\") as in_file:\n",
    "                reader = csv.DictReader(in_file)\n",
    "\n",
    "                # Initialize information trackers\n",
    "                username = None                \n",
    "                (type_counter, total_tweets, hashtag_counter, media_counter, link_counter) = \\\n",
    "                    initialize_trackers()\n",
    "                \n",
    "                # Iterate through Tweets in file\n",
    "                for row in reader: \n",
    "                    \n",
    "                    # Check if this Tweet is from the same user as the previous one\n",
    "                    previous_username = username\n",
    "                    username = row[username_col]\n",
    "                    if previous_username is not None and previous_username != username:\n",
    "                            \n",
    "                        # When moving to a new user, save the results for the previous user\n",
    "                        user_results = calculate_user_results(previous_username, type_counter, total_tweets, \n",
    "                                                              hashtag_counter, media_counter, link_counter)\n",
    "                        writer.writerow(user_results)\n",
    "\n",
    "                        # Reset information trackers for the new user\n",
    "                        (type_counter, total_tweets, hashtag_counter, media_counter, link_counter) = \\\n",
    "                            initialize_trackers()\n",
    "\n",
    "                    # Count types and tokens in the current Tweet\n",
    "                    (type_counter, total_tweets, hashtag_counter, media_counter, link_counter) = \\\n",
    "                        update_trackers(row, type_counter, total_tweets, hashtag_counter, media_counter, link_counter)\n",
    "\n",
    "                # Save the results for the final user\n",
    "                user_results = calculate_user_results(username, type_counter, total_tweets, \n",
    "                                                      hashtag_counter, media_counter, link_counter)\n",
    "                writer.writerow(user_results)\n",
    "\n",
    "def initialize_trackers():\n",
    "    \"\"\"Returns empty trackers to count types, hashtags, media, links, and tweets\"\"\"\n",
    "    type_counter = Counter()\n",
    "    hashtag_counter = Counter()\n",
    "    media_counter = Counter()\n",
    "    link_counter = Counter()\n",
    "    total_tweets = 0\n",
    "    return (type_counter, total_tweets, hashtag_counter, media_counter, link_counter)\n",
    "                \n",
    "def update_trackers(row, type_counter, total_tweets, hashtag_counter, media_counter, link_counter):\n",
    "    \"\"\"Updates the trackers to incorporate a Tweet.\n",
    "    Note: media and links are all conflated to tokens of a single type.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    row: dict; a row of the input CSV, representing a Tweet\n",
    "    type_counter: Counter; a counter over the types used by this user\n",
    "    total_tweets: int; the number of Tweets analyzed for this user\n",
    "    hashtag_counter: Counter; a counter over the hashtag types used by this user\n",
    "    media_counter: Counter; a counter over the media used by this user\n",
    "    link_counter: Counter; a counter over the links used by this user\n",
    "    \"\"\"\n",
    "    tweet = row[\"text\"]\n",
    "    words = extract_words(tweet)\n",
    "    type_counter.update(words)\n",
    "\n",
    "    hashtags = row[\"hashtags\"]\n",
    "    tags = extract_words(hashtags)\n",
    "    hashtag_counter.update(tags)\n",
    "\n",
    "    media = row[\"media_expanded_url\"].split()\n",
    "    media_conflated = [\"MEDIA\"] * len(media)\n",
    "    media_counter.update(media_conflated)\n",
    "\n",
    "    links = row[\"urls_expanded_url\"].split()\n",
    "    links_conflated = [\"LINK\"] * len(links)\n",
    "    link_counter.update(links_conflated)\n",
    "    \n",
    "    total_tweets += 1\n",
    "    \n",
    "    return (type_counter, total_tweets, hashtag_counter, media_counter, link_counter)\n",
    "\n",
    "\n",
    "def calculate_user_results(username, type_counter, total_tweets, hashtag_counter, \n",
    "                           media_counter, link_counter):\n",
    "    \"\"\"Calculates the results for a user, based on completed counters\"\"\"\n",
    "    total_media = sum(media_counter.values())\n",
    "    total_links = sum(link_counter.values())\n",
    "    type_count = len(type_counter) + len(media_counter) + len(link_counter)\n",
    "    token_count = sum(type_counter.values()) + total_media + total_links\n",
    "    ratio = type_count / token_count\n",
    "\n",
    "    hashtag_type = len(hashtag_counter)\n",
    "    hashtag_token = sum(hashtag_counter.values())\n",
    "    if hashtag_token > 0:\n",
    "        hashtag_ratio = hashtag_type / hashtag_token\n",
    "    else:\n",
    "        hashtag_ratio = \"\"\n",
    "\n",
    "    return [username, type_count, token_count, ratio, total_tweets,\n",
    "            hashtag_type, hashtag_token, hashtag_ratio, total_media, total_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c6564-d5fc-4de4-9971-ed8f243c9a6c",
   "metadata": {},
   "source": [
    "The code cell below demonstrates how to use this function to generate user TTR statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07787f1-1a36-4954-8f8c-39e7a830eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob.glob to get a list of all of the input CSV files,\n",
    "# based on a template for their path/filename\n",
    "timeline_data = glob.glob(\"timeline_chunks/timelines_*.csv\")  \n",
    "\n",
    "# Calculate type:token ratios and save them as \"user_ttr_statistics.csv\"\n",
    "type_token(timeline_data, \"user_ttr_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99454d1d-dcbb-4a42-ad65-8f6fa100e237",
   "metadata": {},
   "source": [
    "It is useful to get a summary of the data, to inform filtering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503dd11-ea59-4883-9404-a367e04a2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output as a pandas dataframe\n",
    "timeline_statistics = pd.read_csv(\"user_ttr_statistics.csv\", encoding = \"utf-8\")\n",
    "\n",
    "# Inspect your results\n",
    "timeline_statistics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a8b28-10c4-47b2-a1d3-fca50bafee78",
   "metadata": {},
   "source": [
    "One way to filter users is based on their overall productivity, as indicated by the number of Tweets on their timeline. Since the timeline contains all (publicly-available) Tweets posted by the user over the history of their account, users who do not have many Tweets on their timeline are not productive posters and thus cannot be taken to be reflective of broader community norms.\n",
    "\n",
    "In our data, 25% of users had fewer than 196 Tweets on their timeline; in order to focus on users who are maximally productive, we chose to exclude these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac20a7-8c63-4404-b3d2-d19e1bacef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_post_threshold = 196 # Can be changed\n",
    "users_to_filter.update(timeline_statistics.loc[timeline_statistics[\"TOTAL_TWEETS\"] < post_threshold][\"USERNAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c39ac-8585-4671-9317-ba5a9be89877",
   "metadata": {},
   "source": [
    "Another way to filter users is based on their TTR, as discussed above. For this, it is useful to visualize the distribution of TTR values across the users, to identify a TTR threshold that seems like it would not cut off regular users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ddd85-0bb8-483b-91e2-22c35c54a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histogram showing the distribution of TTR across users\n",
    "timeline_statistics.hist(column=\"RATIO\", bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df11c0-351b-4246-9ac8-6eba7c78ecec",
   "metadata": {},
   "source": [
    "In our data, the majority of users have TTR values greater than 0.2. We decided to exclude users with a TTR of less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d1d91-faa1-4853-aec7-b37a57f145e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_threshold = 0.2 # Can be changed\n",
    "users_to_filter.update(timeline_statistics.loc[timeline_statistics[\"RATIO\"] < ttr_threshold][\"USERNAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2a5d3-b285-4b3e-94b8-aaa011e39495",
   "metadata": {},
   "source": [
    "## Filtering on the basis of Tweets\n",
    "\n",
    "To facilitate filtering on the basis of Tweets, we will keep a set of Tweet IDs that we need to filter out. We'll update this set as we build our filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd84aa-7990-48d2-86f3-d2d64dc0f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_filter = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657db02e-ab08-4dbf-9f8b-0e289bc92937",
   "metadata": {},
   "source": [
    "### Filtering out Tweets with NSFW images\n",
    "\n",
    "In our analysis, we used a [NSFW image detector](https://github.com/GantMan/nsfw_model) to evaluate the probability that a Tweet contains NSFW images. To use this on your own data, you will need to install the image detector (v1.2) and then use our script `classify_images.py` on the JSONL file(s) you harvested from Twitter. This will yield a CSV file with the following columns:  \n",
    "- `tweet.id`: the unique ID number of the Tweet  \n",
    "- `image`: the URL of the image examined by the detector (only the first image in each Tweet is checked)  \n",
    "- `p_drawing`: the probability that the image is a neutral (SFW) drawing  \n",
    "- `p_hentai`: the probability that the image is a pornographic or sexually explicit (NSFW) drawing  \n",
    "- `p_neutral`: the probability that the image is a neutral (SFW) photograph  \n",
    "- `p_porn`: the probability that the image is a pornographic (NSFW) photograph  \n",
    "- `p_sexy`: the probability that the image is a sexually explicit (NSFW) photograph (but not necessarily pornographic in nature)  \n",
    "\n",
    "These probabilities form a distribution: for any image, the probabilities all add up to 1.\n",
    "\n",
    "Based on inspection of our data, we decided in our analysis to exclude any Tweets where either `p_hentai`, `p_porn`, or `p_sexy` was 0.6 or higher. The following code cell demonstrates how the data can be loaded into the exclusion set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e733d0f-7065-4e98-9231-0ec0b1bea105",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"image_probs.csv\", encoding = \"utf-8\") as in_file:\n",
    "    reader = csv.DictReader(in_file)\n",
    "    \n",
    "    for row in reader:\n",
    "        p_hentai = float(row[\"p_hentai\"])\n",
    "        p_porn = float(row[\"p_porn\"])\n",
    "        p_sexy = float(row[\"p_sexy\"])\n",
    "        tweet_id = row[\"tweet.id\"]\n",
    "    \n",
    "        # The criteria below can be changed\n",
    "        if p_hentai >= 0.6 or p_porn >= 0.6 or p_sexy >= 0.6:\n",
    "            tweets_to_filter.add(tweet_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d54d74-d98c-411f-b76d-a3da40df0360",
   "metadata": {},
   "source": [
    "### Filtering out Tweets on the basis of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65118af4-9da5-4799-9593-7167b8c15dae",
   "metadata": {},
   "source": [
    "The script `count_tweet_words.py` can be used to get counts of all the unique words in a CSV file of Tweets. Manual inspection of the wordlist can reveal common terms that seem to indicate material that should be filtered out (especially if using the `--keep-links` option, to keep links in the Tweet text). These terms, or *stopwords*, can be saved to a .txt file (with one term per line), and the following code cell can be used to identify Tweets containing any of these terms and mark them for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674044e6-dd5b-40bc-8ce0-af33a3895c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the path to the stopword file:\n",
    "stopwords_path = \"stopwords.txt\"\n",
    "\n",
    "# Enter the paths to the Tweet CSVs, which contain columns for tweet.id and tweet.text\n",
    "tweet_csvs = [\"study.csv\", \"reference.csv\"]\n",
    "\n",
    "# Read the stopwords into a set\n",
    "stopwords = set()\n",
    "with open(stopwords_path, encoding=\"utf-8\") as in_file:\n",
    "    for line in in_file:\n",
    "        stopwords.add(line.strip())\n",
    "\n",
    "# Find stopwords in Tweets and use them to catch Tweet IDs\n",
    "for tweet_filepath in tweet_csvs:\n",
    "    with open(tweet_filepath, encoding=\"utf-8\") as in_file:\n",
    "        reader = csv.DictReader(tweet_filepath)\n",
    "        \n",
    "        for row in reader:\n",
    "            tweet_id = row[\"tweet.id\"]\n",
    "            tweet_terms = extract_words(row[\"tweet.text\"], remove_links=False)\n",
    "            if stopwords.intersection(tweet_terms):\n",
    "                tweets_to_filter.add(tweet_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da213d2d-bb96-4581-8adf-36965d190bba",
   "metadata": {},
   "source": [
    "In our analysis, manual inspection of the data revealed that a large proportion of Tweets containing the query terms #bi, #bisexuals, #bisexuality, and #bisexualpride contained sexually explicit content, and Tweets containing the query term #bi were also drawn from two irrelevant domains (Business Intelligence, and the Korean rapper B.I). We split our study corpus into NSFW Tweets, which contained any of these query terms, and SFW Tweets, which did not. We used `count_tweet_words.py` (with the `--keep-links` option) to count how many times each word occurred in each subset of the corpus, from which we calculated a NSFW:SFW ratio for that term. We marked as stopwords all words that occurred at least 100 times in the NSFW subset and had a NSFW:SFW ratio of at least 224 (which was the ratio for #bi, the query term with the highest NSFW:SFW ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64346489-e72e-4885-9093-7231bae61925",
   "metadata": {},
   "source": [
    "## Labeling Tweets for filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f3f5d-29d6-4478-b1fa-5efbbed63168",
   "metadata": {},
   "source": [
    "Now we have a set of usernames and Tweet IDs to use for filtering. The final step is to use these sets to assign a label to each Tweet, indicating the level at which it should be filtered out. We use the following labels:  \n",
    "- `user-excluded`: Tweets that are filtered out because they were posted by users who are in `users_to_filter`  \n",
    "- `tweet-excluded`: Tweets that survive user filtering, but are filtered out because they have an ID in `tweets_to_filter`  \n",
    "- `included`: Tweets that survive user and Tweet filtering  \n",
    "\n",
    "In our analysis, we designate Tweets with the *included* label as the *Tweet-filtered* data (i.e., the data after filtering to the Tweet level), and Tweets with the *included* or *tweet-excluded* labels as the *user-filtered* data (i.e., the data after filtering to the user level).\n",
    "\n",
    "We first define a function that takes a row of a Tweet CSV as input and returns a label for it. The ordering is important here: the highest-level filtering should be applied first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320fb0b-3aa6-4430-87ec-fd37cb49e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tweet(row, users_to_filter, tweets_to_filter):\n",
    "    \"\"\"Returns a label (str) based on the stage at which a Tweet\n",
    "    should be filtered out of the data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    row: dict; a row of the Tweet CSV file, mapping from column\n",
    "         headings to column values\n",
    "    users_to_filter: set(str); a set of usernames to filter out\n",
    "    tweets_to_filter: set(str); a set of Tweet IDs to filter out\n",
    "    \"\"\"\n",
    "    if row[\"user.username\"] in users_to_filter:\n",
    "        return \"user-excluded\"\n",
    "    elif row[\"tweet.id\"] in tweets_to_filter:\n",
    "        return \"tweet-excluded\"\n",
    "    else:\n",
    "        return \"included\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa646f-591f-4224-ad9a-61a7e7697edb",
   "metadata": {},
   "source": [
    "The following code cell labels each Tweet in the Tweet CSVs, saving the results in new CSV files with the `_labeled` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7137a7b-a8f1-44b8-b2ac-5cee01a2e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the paths to the Tweet CSVs\n",
    "tweet_csvs = [\"study.csv\", \"reference.csv\"]\n",
    "\n",
    "# Read each CSV file and write a labeled version\n",
    "for tweet_filepath in tweet_csvs:\n",
    "    with open(tweet_filepath, encoding=\"utf-8\") as in_file:\n",
    "        reader = csv.DictReader(tweet_filepath)\n",
    "        \n",
    "        with open(tweet_filepath.replace(\".csv\", \"_labeled.csv\"), \"w\", encoding=\"utf-8\", newline=\"\") as out_file:\n",
    "            fields = reader.fieldnames + [\"label\"]\n",
    "            writer = csv.DictWriter(out_file, fields)\n",
    "            writer.writeheader()\n",
    "        \n",
    "            for row in reader:\n",
    "                row[\"label\"] = label_tweet(row, users_to_filter, tweets_to_filter)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b928e-c255-4b3a-8a95-3f88a3ff7e85",
   "metadata": {},
   "source": [
    "The labeled Tweets can be paired across corpora using `align_corpora.py`, as in the following:\n",
    "\n",
    "```\n",
    "python align_corpora.py study_labeled.csv reference_labeled.csv paired.csv --label-hierarchy included tweet-excluded user-excluded\n",
    "```\n",
    "\n",
    "Notice that the labels are provided in hierarchical order from lowest- to highest-level (separated by spaces), using the `--label-hierarchy` argument. This will create a file `paired.csv` that pairs each study Tweet with a reference Tweet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
